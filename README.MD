# Medical Chatbot (Local RAG + Ollama + Chroma)

A lightweight, fully local medical Q\&A chatbot.

It uses **Ollama** to run an LLM on your machine, **Chroma** as a local vector DB, and **HuggingFace** sentence embeddings for fast semantic search over your PDFs.

### Why this setup?

  * **Local-first**: No cloud keys needed. Your PDFs never leave your machine.
  * **Chroma**: Simple, embedded vector DB with disk persistence‚Äîideal for local projects.
  * **HuggingFace embeddings**: Fast CPU model (**all-MiniLM-L6-v2**, 384-dim) keeps latency low.
  * **Ollama**: Runs models like **mistral** or **llama3** locally with one command.
  * **Stateful RAG**: We use LangChain‚Äôs `RunnableWithMessageHistory` to remember conversation context (follow-ups work).
  * **Intent routing**: Greets naturally, handles small-talk, and gracefully falls back when the query is off-topic.
  * **Readable output**: The LLM is prompted to return **HTML fragments** using paragraphs and bullet points for clear UI rendering.

-----

### Project Structure

```bash
medical_chatbot/
‚îú‚îÄ app.py                 # Flask app (stateful RAG + intent router)
‚îú‚îÄ store_index.py         # Build/update local Chroma index from PDFs
‚îú‚îÄ requirements.txt
‚îú‚îÄ Dockerfile
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îî‚îÄ helper.py           # PDF loader, splitter, embeddings helper (if used)
‚îú‚îÄ data/                  # Put your PDFs here
‚îú‚îÄ chroma_db/             # Created by store_index.py (persisted index)
‚îú‚îÄ templates/
‚îÇ  ‚îî‚îÄ chat.html           # Your chat UI (uses your CSS)
‚îî‚îÄ static/
   ‚îî‚îÄ style.css           # Dark chat theme
```

-----

### Prerequisites

  * **Python** 3.11 (recommended for compatibility)
  * `pip` (or conda)
  * **Ollama** installed: `https://ollama.com/`

After install, pull a model (e.g., `mistral`):

```bash
ollama pull mistral
```

-----

### 1\) Install dependencies (local)

```bash
# From project root
pip install -r requirements.txt
```

(If you use conda, activate your env first.)

-----

### 2\) Build the semantic index (once per PDF update)

Put your PDFs in `./data/`, then run:

```bash
python store_index.py
```

You should see logs like:

```
üìÅ Loading PDFs from: data/
‚úÇÔ∏è Prepared <N> chunks.
üóÑÔ∏è Writing to Chroma at: ./chroma_db (collection: medical-chatbot)
‚úÖ Chroma index created/updated and persisted.
```

This creates a **local** Chroma collection at `./chroma_db` that `app.py` connects to.

-----

### 3\) Start Ollama (LLM backend)

Open a terminal and run:

```bash
ollama serve
```

Make sure the model is available:

```bash
ollama pull mistral
```

You can switch models in `app.py` via `LLM_MODEL`.

-----

### 4\) Run the web app (local)

In another terminal:

```bash
python app.py
```

Open `http://127.0.0.1:8080/` in your browser.

#### How the UI behaves

  * **Greetings / small‚Äëtalk** (‚Äúhi‚Äù, ‚Äúhow are you?‚Äù) ‚Üí friendly reply (no medical context).
  * **On-topic medical queries** ‚Üí **stateful RAG** over your PDFs (follow‚Äëups like ‚Äúelaborate‚Äù work).
  * **Out‚Äëof‚Äëdomain queries** ‚Üí conversational fallback (won‚Äôt hallucinate from random medical chunks).
  * **End/reset** ‚Üí send `reset`, `end`, `stop`, or `clear`. This clears just the current session‚Äôs memory.

The frontend can pass a `session_id` with each request so multiple tabs/users have independent memory.

**Example jQuery AJAX (already in your HTML):**

```javascript
$.ajax({
  data: {
    msg: rawText,
    session_id: 'default' // or generate a UUID per tab
  },
  type: "POST",
  url: "/get"
});
```

-----

### 5\) Run with Docker (optional)

**Build:**

```bash
docker build -t medical-chatbot .
```

**Run:**

```bash
docker run -p 8080:8080 medical-chatbot
```

Open `http://127.0.0.1:8080/`

**Note**: This container runs only the Flask app. **Ollama must be running on the host** (`ollama serve`) so the app can contact it.

-----

### Config knobs (quick)

  * **Model**: `LLM_MODEL` in `app.py` (e.g., `"mistral"`, `"llama3.1:8b"`).
  * **Retriever**: MMR used to reduce redundancy:
    ```python
    retriever = docsearch.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 4, "fetch_k": 20, "lambda_mult": 0.5}
    )
    ```
  * **Relevance gate**: `RELEVANCE_THRESHOLD = 0.30` in `app.py`. Lower = stricter; typical range `0.25‚Äì0.40`. Prevents pulling random context.
  * **HTML output**: System prompt tells the model to return **HTML fragments** (`<p>`, `<ul><li>`), improving readability in your chat UI.
  * **Session memory**: `RunnableWithMessageHistory` + `MessagesPlaceholder("history")`. Each request should include a `session_id`.

-----

### Troubleshooting

  * **No answers / random text for ‚Äúhi‚Äù**
    Make sure the intent router is present (we bypass RAG for small‚Äëtalk).
  * **No documents retrieved**
    Verify `./data/` has PDFs, `store_index.py` prepared chunks, and both `app.py` and `store_index.py` point to the same:
    ```python
    PERSIST_DIR = "./chroma_db"
    COLLECTION = "medical-chatbot"
    ```
  * **Model/LLM errors**
    Ensure `ollama serve` is running and the model is pulled: `ollama pull mistral`.
  * **Install errors (Docker or local)**
    Stick to **Python 3.11** for best compatibility with `chromadb`, `langchain-*`, and `sentence-transformers`.

-----

### Reasoning: why we made these choices

  * **Local privacy**: PDFs never leave your machine (Ollama + Chroma are local).
  * **Performance & simplicity**: MiniLM embeddings are fast on CPU; Chroma persists to disk automatically.
  * **Good UX**: Intent routing avoids awkward ‚ÄúHi ‚Üí random medical summary‚Äù. Relevance gating reduces off-topic hallucinations.
  * **Maintainability**: Uses the latest LangChain interfaces (`langchain_chroma`, `langchain_huggingface`, `RunnableWithMessageHistory`) to avoid deprecations and ease upgrades.
  * **Readable answers**: HTML fragments (paragraphs & bullets) render nicely in your chat UI and expand automatically on ‚Äúelaborate‚Äù.

-----

### One-liners

**Rebuild index then run app:**

```bash
python store_index.py && python app.py
```

**Quick curl test:**

```bash
curl -X POST -d "msg=What does the context say about acromegaly?" -d "session_id=default" http://127.0.0.1:8080/get
```

**Reset conversation:**

```bash
curl -X POST -d "msg=reset" -d "session_id=default" http://127.0.0.1:8080/get
```